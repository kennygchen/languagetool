


<!DOCTYPE html>
<html id="htmlId">
<head>
  <meta http-equiv="Content-Type" content="text/html;charset=UTF-8"> 
  <title>Coverage Report > CommonCrawlToNgram</title>
  <style type="text/css">
    @import "../../css/coverage.css";
    @import "../../css/idea.min.css";
  </style>
  <script type="text/javascript" src="../../js/highlight.min.js"></script>
  <script type="text/javascript" src="../../js/highlightjs-line-numbers.min.js"></script>
</head>

<body>
<div class="content">
<div class="breadCrumbs">
Current scope:     <a href="../../index.html">all classes</a>
    <span class="separator">|</span>
    <a href="../index.html">org.languagetool.dev.bigdata</a>
</div>

<h1>Coverage Summary for Class: CommonCrawlToNgram (org.languagetool.dev.bigdata)</h1>

<table class="coverageStats">

<tr>
  <th class="name">Class</th>
<th class="coverageStat 
">
  Method, %
</th>
<th class="coverageStat 
">
  Branch, %
</th>
<th class="coverageStat 
">
  Line, %
</th>
</tr>
<tr>
  <td class="name">CommonCrawlToNgram</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/12)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/54)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/122)
  </span>
</td>
</tr>
  <tr>
    <td class="name">CommonCrawlToNgram$LuceneLiveIndex</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/2)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/10)
  </span>
</td>
  </tr>
<tr>
  <td class="name"><strong>Total</strong></td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/14)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/54)
  </span>
</td>
<td class="coverageStat">
  <span class="percent">
    0%
  </span>
  <span class="absValue">
    (0/132)
  </span>
</td>
</tr>
</table>

<br/>
<br/>


<pre>
<code class="sourceCode" id="sourceCode">&nbsp;/* LanguageTool, a natural language style checker 
&nbsp; * Copyright (C) 2015 Daniel Naber (http://www.danielnaber.de)
&nbsp; * 
&nbsp; * This library is free software; you can redistribute it and/or
&nbsp; * modify it under the terms of the GNU Lesser General Public
&nbsp; * License as published by the Free Software Foundation; either
&nbsp; * version 2.1 of the License, or (at your option) any later version.
&nbsp; *
&nbsp; * This library is distributed in the hope that it will be useful,
&nbsp; * but WITHOUT ANY WARRANTY; without even the implied warranty of
&nbsp; * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
&nbsp; * Lesser General Public License for more details.
&nbsp; *
&nbsp; * You should have received a copy of the GNU Lesser General Public
&nbsp; * License along with this library; if not, write to the Free Software
&nbsp; * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA  02110-1301
&nbsp; * USA
&nbsp; */
&nbsp;package org.languagetool.dev.bigdata;
&nbsp;
&nbsp;import org.apache.lucene.analysis.Analyzer;
&nbsp;import org.apache.lucene.analysis.standard.StandardAnalyzer;
&nbsp;import org.apache.lucene.document.*;
&nbsp;import org.apache.lucene.index.*;
&nbsp;import org.apache.lucene.search.IndexSearcher;
&nbsp;import org.apache.lucene.search.TermQuery;
&nbsp;import org.apache.lucene.search.TopDocs;
&nbsp;import org.apache.lucene.store.Directory;
&nbsp;import org.apache.lucene.store.FSDirectory;
&nbsp;import org.jetbrains.annotations.NotNull;
&nbsp;import org.languagetool.Language;
&nbsp;import org.languagetool.Languages;
&nbsp;import org.languagetool.dev.eval.SimpleCorpusEvaluator;
&nbsp;import org.languagetool.languagemodel.LanguageModel;
&nbsp;import org.languagetool.rules.en.GoogleStyleWordTokenizer;
&nbsp;import org.languagetool.tokenizers.SentenceTokenizer;
&nbsp;import org.languagetool.tokenizers.Tokenizer;
&nbsp;import org.tukaani.xz.XZInputStream;
&nbsp;
&nbsp;import java.io.*;
&nbsp;import java.util.HashMap;
&nbsp;import java.util.List;
&nbsp;import java.util.Locale;
&nbsp;import java.util.Map;
&nbsp;
&nbsp;/**
&nbsp; * Indexing the CommonCrawl-based data from http://data.statmt.org/ngrams/
&nbsp; * to ngrams.
&nbsp; * 
&nbsp; * @since 3.2
&nbsp; */
&nbsp;class CommonCrawlToNgram implements AutoCloseable {
&nbsp;
&nbsp;  private static final double THRESHOLD = 0.00000000001;
&nbsp;  private static final int MAX_TOKEN_LENGTH = 20;
&nbsp;  
&nbsp;  private final File input;
&nbsp;  private final File indexTopDir;
&nbsp;  private final File evalFile;
&nbsp;  private final SentenceTokenizer sentenceTokenizer;
&nbsp;  private final Tokenizer wordTokenizer;
<b class="nc">&nbsp;  private final Map&lt;String, Long&gt; unigramToCount = new HashMap&lt;&gt;();</b>
<b class="nc">&nbsp;  private final Map&lt;String, Long&gt; bigramToCount = new HashMap&lt;&gt;();</b>
<b class="nc">&nbsp;  private final Map&lt;String, Long&gt; trigramToCount = new HashMap&lt;&gt;();</b>
<b class="nc">&nbsp;  private final Map&lt;Integer, LuceneLiveIndex&gt; indexes = new HashMap&lt;&gt;();</b>
&nbsp;  
<b class="nc">&nbsp;  private int cacheLimit = 1_000_000;  // max. number of trigrams in HashMap before we flush to Lucene</b>
<b class="nc">&nbsp;  private long charCount = 0;</b>
<b class="nc">&nbsp;  private long lineCount = 0;</b>
&nbsp;
<b class="nc">&nbsp;  CommonCrawlToNgram(Language language, File input, File indexTopDir, File evalFile) throws IOException {</b>
<b class="nc">&nbsp;    this.input = input;</b>
<b class="nc">&nbsp;    this.indexTopDir = indexTopDir;</b>
<b class="nc">&nbsp;    this.evalFile = evalFile;</b>
<b class="nc">&nbsp;    this.sentenceTokenizer = language.getSentenceTokenizer();</b>
<b class="nc">&nbsp;    this.wordTokenizer = new GoogleStyleWordTokenizer();</b>
<b class="nc">&nbsp;    indexes.put(1, new LuceneLiveIndex(new File(indexTopDir, &quot;1grams&quot;)));</b>
<b class="nc">&nbsp;    indexes.put(2, new LuceneLiveIndex(new File(indexTopDir, &quot;2grams&quot;)));</b>
<b class="nc">&nbsp;    indexes.put(3, new LuceneLiveIndex(new File(indexTopDir, &quot;3grams&quot;)));</b>
&nbsp;  }
&nbsp;  
&nbsp;  @Override
&nbsp;  public void close() throws IOException {
<b class="nc">&nbsp;    for (LuceneLiveIndex index : indexes.values()) {</b>
&nbsp;      index.close();
&nbsp;    }
&nbsp;  }
&nbsp;
&nbsp;  void setCacheLimit(int cacheLimit) {
<b class="nc">&nbsp;    this.cacheLimit = cacheLimit;</b>
&nbsp;  }
&nbsp;  
&nbsp;  void indexInputFile() throws IOException {
<b class="nc">&nbsp;    writeAndEvaluate();  // run now so we have a baseline</b>
<b class="nc">&nbsp;    FileInputStream fin = new FileInputStream(input);</b>
<b class="nc">&nbsp;    BufferedInputStream in = new BufferedInputStream(fin);</b>
<b class="nc">&nbsp;    try (XZInputStream xzIn = new XZInputStream(in)) {</b>
<b class="nc">&nbsp;      final byte[] buffer = new byte[8192];</b>
&nbsp;      int n;
<b class="nc">&nbsp;      while ((n = xzIn.read(buffer)) != -1) {</b>
<b class="nc">&nbsp;        String buf = new String(buffer, 0, n);  // TODO: not always correct, we need to wait for line end first?</b>
<b class="nc">&nbsp;        String[] lines = buf.split(&quot;\n&quot;);</b>
<b class="nc">&nbsp;        indexLine(lines);</b>
&nbsp;      }
&nbsp;    }
<b class="nc">&nbsp;    writeAndEvaluate();</b>
&nbsp;  }
&nbsp;
&nbsp;  private void indexLine(String[] lines) throws IOException {
<b class="nc">&nbsp;    for (String line : lines) {</b>
<b class="nc">&nbsp;      if (lineCount++ % 50_000 == 0) {</b>
<b class="nc">&nbsp;        float mb = (float) charCount / 1000 / 1000;</b>
<b class="nc">&nbsp;        System.out.printf(Locale.ENGLISH, &quot;Indexing line %d (%.2fMB)\n&quot;, lineCount, mb);</b>
&nbsp;      }
<b class="nc">&nbsp;      charCount += line.length();</b>
<b class="nc">&nbsp;      List&lt;String&gt; sentences = sentenceTokenizer.tokenize(line);</b>
<b class="nc">&nbsp;      for (String sentence : sentences) {</b>
<b class="nc">&nbsp;        indexSentence(sentence);</b>
&nbsp;      }
&nbsp;    }
&nbsp;  }
&nbsp;
&nbsp;  private void indexSentence(String sentence) throws IOException {
<b class="nc">&nbsp;    List&lt;String&gt; tokens = wordTokenizer.tokenize(sentence);</b>
<b class="nc">&nbsp;    tokens.add(0, LanguageModel.GOOGLE_SENTENCE_START);</b>
<b class="nc">&nbsp;    tokens.add(LanguageModel.GOOGLE_SENTENCE_END);</b>
<b class="nc">&nbsp;    String prevPrev = null;</b>
<b class="nc">&nbsp;    String prev = null;</b>
<b class="nc">&nbsp;    for (String token : tokens) {</b>
<b class="nc">&nbsp;      if (token.trim().isEmpty()) {</b>
&nbsp;        continue;
&nbsp;      }
<b class="nc">&nbsp;      if (token.length() &lt;= MAX_TOKEN_LENGTH) {</b>
<b class="nc">&nbsp;        unigramToCount.compute(token, (k, v) -&gt; v == null ? 1 : v + 1);</b>
&nbsp;      }
<b class="nc">&nbsp;      if (prev != null) {</b>
<b class="nc">&nbsp;        if (token.length() &lt;= MAX_TOKEN_LENGTH &amp;&amp; prev.length() &lt;= MAX_TOKEN_LENGTH) {</b>
<b class="nc">&nbsp;          String ngram = prev + &quot; &quot; + token;</b>
<b class="nc">&nbsp;          bigramToCount.compute(ngram, (k, v) -&gt; v == null ? 1 : v + 1);</b>
&nbsp;        }
&nbsp;      }
<b class="nc">&nbsp;      if (prevPrev != null &amp;&amp; prev != null) {</b>
<b class="nc">&nbsp;        if (token.length() &lt;= MAX_TOKEN_LENGTH &amp;&amp; prev.length() &lt;= MAX_TOKEN_LENGTH &amp;&amp; prevPrev.length() &lt;= MAX_TOKEN_LENGTH) {</b>
<b class="nc">&nbsp;          String ngram = prevPrev + &quot; &quot; + prev + &quot; &quot; + token;</b>
<b class="nc">&nbsp;          trigramToCount.compute(ngram, (k, v) -&gt; v == null ? 1 : v + 1);</b>
&nbsp;        }
<b class="nc">&nbsp;        if (trigramToCount.size() &gt; cacheLimit) {</b>
<b class="nc">&nbsp;          writeAndEvaluate();</b>
&nbsp;        }
&nbsp;      }
<b class="nc">&nbsp;      prevPrev = prev;</b>
<b class="nc">&nbsp;      prev = token;</b>
&nbsp;    }
&nbsp;  }
&nbsp;
&nbsp;  private void writeAndEvaluate() throws IOException {
<b class="nc">&nbsp;    writeToLucene(1, unigramToCount);</b>
<b class="nc">&nbsp;    writeToLucene(2, bigramToCount);</b>
<b class="nc">&nbsp;    writeToLucene(3, trigramToCount);</b>
<b class="nc">&nbsp;    if (evalFile != null) {</b>
<b class="nc">&nbsp;      System.out.println(&quot;Running evaluation...&quot;);</b>
<b class="nc">&nbsp;      long startTime = System.currentTimeMillis();</b>
<b class="nc">&nbsp;      SimpleCorpusEvaluator evaluator = new SimpleCorpusEvaluator(indexTopDir);</b>
<b class="nc">&nbsp;      evaluator.run(evalFile, THRESHOLD);</b>
<b class="nc">&nbsp;      System.out.println(&quot;Eval time: &quot; + (System.currentTimeMillis()-startTime) + &quot;ms&quot;);</b>
&nbsp;    } else {
<b class="nc">&nbsp;      System.out.println(&quot;Skipping evaluation, no evaluation file specified&quot;);</b>
&nbsp;    }
&nbsp;  }
&nbsp;  
&nbsp;  private void writeToLucene(int ngramSize, Map&lt;String, Long&gt; ngramToCount) throws IOException {
<b class="nc">&nbsp;    long startTime = System.currentTimeMillis();</b>
<b class="nc">&nbsp;    System.out.println(&quot;Writing &quot; + ngramToCount.size() + &quot; cached ngrams to Lucene index (ngramSize=&quot; + ngramSize + &quot;)...&quot;);</b>
<b class="nc">&nbsp;    LuceneLiveIndex index = indexes.get(ngramSize);</b>
&nbsp;    // not sure why this doesn&#39;t work, should be faster:
&nbsp;    /*DirectoryReader newReader = DirectoryReader.openIfChanged(reader);
&nbsp;    if (newReader != null) {
&nbsp;      reader = newReader;
&nbsp;    }*/
<b class="nc">&nbsp;    index.reader = DirectoryReader.open(index.indexWriter, true);</b>
<b class="nc">&nbsp;    index.searcher = new IndexSearcher(index.reader);</b>
<b class="nc">&nbsp;    for (Map.Entry&lt;String, Long&gt; entry : ngramToCount.entrySet()) {</b>
<b class="nc">&nbsp;      Term ngram = new Term(&quot;ngram&quot;, entry.getKey());</b>
<b class="nc">&nbsp;      TopDocs topDocs = index.searcher.search(new TermQuery(ngram), 2);</b>
&nbsp;      //System.out.println(ngram + &quot; ==&gt; &quot; + topDocs.totalHits);
<b class="nc">&nbsp;      if (topDocs.totalHits == 0) {</b>
<b class="nc">&nbsp;        Document doc = getDoc(entry.getKey(), entry.getValue());</b>
<b class="nc">&nbsp;        index.indexWriter.addDocument(doc);</b>
<b class="nc">&nbsp;      } else if (topDocs.totalHits == 1) {</b>
<b class="nc">&nbsp;        int docNumber = topDocs.scoreDocs[0].doc;</b>
<b class="nc">&nbsp;        Document document = index.reader.document(docNumber);</b>
<b class="nc">&nbsp;        long oldCount = Long.parseLong(document.getField(&quot;count&quot;).stringValue());</b>
&nbsp;        //System.out.println(ngram + &quot; -&gt; &quot; + oldCount + &quot;+&quot; + entry.getValue());
<b class="nc">&nbsp;        index.indexWriter.deleteDocuments(ngram);</b>
<b class="nc">&nbsp;        index.indexWriter.addDocument(getDoc(entry.getKey(), oldCount + entry.getValue()));</b>
&nbsp;        // would probably be faster, but we currently rely on the count being a common field:
&nbsp;        //indexWriter.updateNumericDocValue(ngram, &quot;count&quot;, oldCount + entry.getValue());
<b class="nc">&nbsp;      } else if (topDocs.totalHits &gt; 1) {</b>
<b class="nc">&nbsp;        throw new RuntimeException(&quot;Got more than one hit for: &quot; + ngram);</b>
&nbsp;      }
&nbsp;      //System.out.println(&quot;   &quot; + entry.getKey() + &quot; -&gt; &quot; + entry.getValue());
&nbsp;    }
<b class="nc">&nbsp;    if (ngramSize == 1) {</b>
&nbsp;      // TODO: runtime code will crash if there are more than 1000 of these docs, so update instead of delete
<b class="nc">&nbsp;      long total = ngramToCount.values().stream().mapToLong(Number::longValue).sum();</b>
<b class="nc">&nbsp;      System.out.println(&quot;Adding totalTokenCount doc: &quot; + total);</b>
<b class="nc">&nbsp;      addTotalTokenCountDoc(total, index.indexWriter);</b>
&nbsp;    }
<b class="nc">&nbsp;    System.out.println(&quot;Commit...&quot;);</b>
<b class="nc">&nbsp;    index.indexWriter.commit();</b>
<b class="nc">&nbsp;    System.out.println(&quot;Commit done, indexing took &quot; + (System.currentTimeMillis()-startTime) + &quot;ms&quot;);</b>
<b class="nc">&nbsp;    ngramToCount.clear();</b>
&nbsp;  }
&nbsp;
&nbsp;  @NotNull
&nbsp;  private Document getDoc(String ngram, long count) {
<b class="nc">&nbsp;    Document doc = new Document();</b>
<b class="nc">&nbsp;    doc.add(new Field(&quot;ngram&quot;, ngram, StringField.TYPE_NOT_STORED));</b>
<b class="nc">&nbsp;    doc.add(getCountField(count));</b>
<b class="nc">&nbsp;    return doc;</b>
&nbsp;  }
&nbsp;
&nbsp;  @NotNull
&nbsp;  private LongField getCountField(long count) {
<b class="nc">&nbsp;    FieldType fieldType = new FieldType();</b>
<b class="nc">&nbsp;    fieldType.setStored(true);</b>
<b class="nc">&nbsp;    fieldType.setOmitNorms(true);</b>
<b class="nc">&nbsp;    fieldType.setNumericType(FieldType.NumericType.LONG);</b>
<b class="nc">&nbsp;    fieldType.setDocValuesType(DocValuesType.NUMERIC);</b>
<b class="nc">&nbsp;    return new LongField(&quot;count&quot;, count, fieldType);</b>
&nbsp;  }
&nbsp;
&nbsp;  private void addTotalTokenCountDoc(long totalTokenCount, IndexWriter writer) throws IOException {
<b class="nc">&nbsp;    FieldType fieldType = new FieldType();</b>
<b class="nc">&nbsp;    fieldType.setIndexOptions(IndexOptions.DOCS);</b>
<b class="nc">&nbsp;    fieldType.setStored(true);</b>
<b class="nc">&nbsp;    fieldType.setOmitNorms(true);</b>
<b class="nc">&nbsp;    Field countField = new Field(&quot;totalTokenCount&quot;, String.valueOf(totalTokenCount), fieldType);</b>
<b class="nc">&nbsp;    Document doc = new Document();</b>
<b class="nc">&nbsp;    doc.add(countField);</b>
<b class="nc">&nbsp;    writer.addDocument(doc);</b>
&nbsp;  }
&nbsp;
&nbsp;  public static void main(String[] args) throws IOException {
<b class="nc">&nbsp;    if (args.length != 4) {</b>
<b class="nc">&nbsp;      System.out.println(&quot;Usage: &quot; + CommonCrawlToNgram.class + &quot; &lt;langCode&gt; &lt;input.xz&gt; &lt;ngramIndexDir&gt; &lt;simpleEvalFile&gt;&quot;);</b>
<b class="nc">&nbsp;      System.out.println(&quot; &lt;simpleEvalFile&gt; a plain text file with simple error markup&quot;);</b>
<b class="nc">&nbsp;      System.exit(1);</b>
&nbsp;    }
<b class="nc">&nbsp;    Language language = Languages.getLanguageForShortCode(args[0]);</b>
<b class="nc">&nbsp;    File input = new File(args[1]);</b>
<b class="nc">&nbsp;    File outputDir = new File(args[2]);</b>
<b class="nc">&nbsp;    File evalFile = new File(args[3]);</b>
<b class="nc">&nbsp;    try (CommonCrawlToNgram prg = new CommonCrawlToNgram(language, input, outputDir, evalFile)) {</b>
<b class="nc">&nbsp;      prg.indexInputFile();</b>
&nbsp;    }
&nbsp;  }
&nbsp;  
&nbsp;  static class LuceneLiveIndex {
&nbsp;
&nbsp;    private final Directory directory;
&nbsp;    private final IndexWriter indexWriter;
&nbsp;
&nbsp;    private DirectoryReader reader;
&nbsp;    private IndexSearcher searcher;
&nbsp;
<b class="nc">&nbsp;    LuceneLiveIndex(File dir) throws IOException {</b>
<b class="nc">&nbsp;      Analyzer analyzer = new StandardAnalyzer();</b>
<b class="nc">&nbsp;      IndexWriterConfig config = new IndexWriterConfig(analyzer);</b>
<b class="nc">&nbsp;      directory = FSDirectory.open(dir.toPath());</b>
<b class="nc">&nbsp;      indexWriter = new IndexWriter(directory, config);</b>
<b class="nc">&nbsp;      reader = DirectoryReader.open(indexWriter, false);</b>
<b class="nc">&nbsp;      searcher = new IndexSearcher(reader);</b>
&nbsp;    }
&nbsp;    
&nbsp;    void close() throws IOException {
<b class="nc">&nbsp;      reader.close();</b>
<b class="nc">&nbsp;      indexWriter.close();</b>
<b class="nc">&nbsp;      directory.close();</b>
&nbsp;    }
&nbsp;
&nbsp;  }
&nbsp;}
</code>
</pre>
</div>

<script type="text/javascript">
(function() {
    var msie = false, msie9 = false;
    /*@cc_on
      msie = true;
      @if (@_jscript_version >= 9)
        msie9 = true;
      @end
    @*/

    if (!msie || msie && msie9) {
      hljs.highlightAll()
      hljs.initLineNumbersOnLoad();
    }
})();
</script>

<div class="footer">
    
    <div style="float:right;">generated on 2025-02-17 23:47</div>
</div>
</body>
</html>
